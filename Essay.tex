\documentclass{article}
\bibliographystyle{ieeetr}
\usepackage{graphicx} 
\usepackage{svg}

\title{Echocardiography Guidance and Training with Mixed Reality}
\author{Edvart G. Bjerke}
\date{March 2025}

\begin{document}
\maketitle

\section{Introduction}
The aim of this thesis is to implement an echocardiography guidance and training system in augmented reality. The system is targeted at trainees, students and other unskilled practitioners. The goal is to provide a tool that is helpful both in learning about how to perform echocardiographic examinations and in actual examinations.

A pre-trained deep neural network- based model will be used to generate guidance signals for the user, and these signals will be integrated into the augmented reality system.  
Additionally, virtual reference model placed in the virtual environment allows the user to explore cardiac structures and familiarize themselves with
standard views. 

The reference models are based on cardiac MRI data. 
Since the project is focused on echocardiography, it's essential that the reference model also includes ultrasound data.
This data should be spatially complete such that the user can view scan planes from arbitrary poses in a continuous manner.
To maintain coherence, all representations (raw MRI, solid model and ultrasound) are obtained from the same patient. This ensures that context is not lost when switching between representations.

Obtaining dense 4D ultrasound data that coheres with the MRI data from manual scanning is impractical. Instead,
echo data is simulated based on the solid model manually generated by contouring the 4D MRI data.

We wish to investigate whether AR technology coupled with a guidance model can enhance the learning experience of novice echocardiography users.
This thesis hypothesizes that such technologies can reduce friction in the learning experience through increased interactivity and reduction of the context-switching
apparent in traditional learning methods. We propose that providing immediate feedback and displaying information directly in the users field of view 
promotes more focused learning where the need to shift attention, both physically and mentally, is reduced. We also aim to investigate if these techniques 
improve the user's ability to understand and internalize the geometry of cardiac structures.




\section{Echocardiography}
Echocardiography examinations provide information about the patient's heart used to diagnose various 
cardiovascular disorders. Today, echocardiography is used extensively, as the procedure is low-cost and non-invasive \cite{Omerovic2023}.
In Transthoracic Echocardiography (TTE), the ultrasound probe is positioned externally- on the chest of the patient.
This is the most common technique for obtaining echocardiograms, and is less invasive than Transesophageal Echocardiography (TEE), where 
a probe is inserted into the esophagus \cite{Omerovic2023}. In this thesis, we are only concerned with TTE.

In echocardiography, ultrasound waves are transmitted by a probe. The waves propagate through the medium of the patient and echoes are recorded
by the probe. These echoes are processed by a set of signal processing algorithms to produce the ultrasound image. A technique known as "beamforming" is an essential part of the ultrasound processing chain. 
There are several imaging \textit{modalities} used in cardiac ultrasound. The most common types are M-mode, 2D and 3D imaging.

In M-mode imaging, only a single ultrasound scan line is processed and displayed. This modality
may be useful when evaluating the motion or timing of a specific structure with respect to the cardiac cycle \cite{SAUL2015686}.
Two-dimensional ultrasound allows the generation of cross-sectional images of the anatomy, and is the most common modality for 
echocardiograms today (CITATION NEEDED). Three-dimensional ultrasound technology produces volumetric data of the anatomy by using two-dimensional transducer arrays.
These volumes can be produced in real-time and displayed using volume rendering techniques like ray-marching or isosurface generation. One key advantage with 3D echo
is that volumes can be estimated directly from measurements reducing errors intrinsic in volume estimation from cross-sectional slices \cite{Shah2013}.
In this thesis, we are mostly concerned with two-dimensional imaging as this is the most available method.

Beamforming technqiques exploit wave interference to combine ultrasound signals from separate transducers. The result of beamforming is a
greyscale image visualizing anatomical structures. More specifically, the intensity of a pixel in the image correlates with the strength of the reflected echo, influenced by differences
in acoustic impedance.

Due to the intricacies of acoustic wave propagation and limitations in beamforming, ultrasound images are generally more difficult to interpret than those obtained by Computed Tomography or Magnetic Resonance.
These images typically contain significant noise and frequently exhibit visual artefacts such as \textit{abberations} and \textit{reverberations}. Echocardiography is specifically limited due to 
rib blockages, and special probes are designed such that the acoustic field of view may pass in between the ribs of the patient.

\subsection{Standard Viewing Planes}
The standard viewing planes are specific standardized cross-sections used to assess cardiac health.
The standard views are the parasternal long axis, the parasternal short axis, the apical four-chamber and 
the subxiphoid \cite{Shah2013}. These views are obtained by placing the probe at specific positions and orientations which requires
training and experience. This paradigm is useful in the sense that it provides clear objectives
for the user to obtain and forms a basis for objective assessment of the quality of the examination.





\section{Computer Graphics}
Computer graphics (CG) is the study of digitally generating images, videos, and other types of graphical content.
The type of visual content generated can broadly be categorized into photorealistic and non-photorealistic CG.
The applications of CG techniques are broad- including scientific visualizations, video games, special effects and other types of informative or artistic content.

Algorithms in computer graphics typically operate on objects of a three-dimensional (3D) \textit{scene}. The scene is a high-level description of the virtual environment used to synthesize the visual content.
The objects in the scene may represent things like solid bodies, cameras, light sources or arbitrary volumetric data.

\subsection{Rendering}
Rendering algorithms generate digital images, given a set of objects from the scene. These algorithms
are often physically-based, where \textit{photorealistic} techniques simulate real light phenomena to produce visually realistic results. 


\textit{Non-photorealistic} rendering techniques are used when photorealism is n    ot desired- whether for aesthetic purposes or when the objects represent something
invisible to the human eye. 

\subsection{The Graphics Pipeline}
The graphics pipeline is a high-level description of the sequence of processes used to 
generate an image from the scene description. The graphics pipeline is not rigidly defined,
and different architectures are used to render different tyepes of data or to achieve results of a particular aesthetic.

In real-time computer graphics, the graphics pipeline can generally be viewed as three parts- the application, geometry processing and rasterization \cite{RealTimeRendering}.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=1.0\textwidth]{images/graphics_pipeline.png}
    \caption{The graphics pipeline \cite{RealTimeRendering}}
    \label{fig:pipelineRealTimeRendering}
\end{figure}

The application part of the pipeline generally runs on the CPU, and is concerned with defining the structure of the scene, setting up rendering commands and other pre-processing procedures.
The application defines a set of geometry \textit{primitives} which are sent further down the pipeline. These primitives
are geometrical shapes like triangles and lines, defined by coordinates and indices defining the surface of the models to be processed and rendered by the GPU.
The data strutures representing these primitives are typically defined in a local coordinate system referred to as \textit{object space}, and may include additional information such as surface normals and texture coordinates. The data required
in this stage depends on the algorithms employed in the \textit{pixel processing} stage of the pipeline.

The geometry processing stage is executed on the GPU, where geometrical operations like morphing and subdividing (tesselation) may be performed.
An essential part of the geometry processing stage is the \textit{vertex shader}, which is a programmable procedure which processes each 3D vertex (position) of the given mesh.

In a typical rendering pipeline, the vertex shader transforms the vertex coordinates from \textit{object space} to \textit{screen space} through perspective projection, such that
objects are correctly rendered to the screen according to the \textit{pinhole camera model}.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=1.0\textwidth]{images/coordinate_systems.png}
    \caption{Representation of geometry processing in a standard vertex shader using perspective projection\cite{learnopengl_figure}.}
    \label{fig:graphicsTransformations}
\end{figure}

The rasterization part of the pipeline uses the screen space coordinates computed by the vertex shader to
determine the set of pixels covered by the projection of the primitive. 

Finally, the colors of the rasterized pixels are determined in the pixel processing stage.
The GPU program defining the algorithm used for shading pixels is referred to as the \textit{fragment shader}.
The fragment shader is executed in parallell, for each rasterized pixel. With modern GPUs and graphics frameworks, fragment shaders are highly programmable, and different shaders may be used to render different objects.

\subsection{Volume Rendering}
The traditional raster-based rendering technique is not well-suited for rendering phenomena that are volumetric in nature.
Special rendering techniques have been developed to render physically occuring phenomena like smoke, clouds or fog. These techniques
may also be used to visualize three-dimensional scientific data, like that captured by fluid simulation or medical imaging.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.5\textwidth]{images/VolumeRendering.png}
    \caption{Volume rendering of a CT scan of a forearm}
    \label{fig:forearmCT}
\end{figure}

\subsubsection{Cut Planes}
A simple way to render volumetric data is reduce the problem to two dimensions, by only rendering a single \textit{slice} of the data
at once. This can be achieved by rendering a plane placed such that it intersects with the bounding volume of the 
data set. 
\begin{figure}[tbh!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/CutPlanes.png}
    \caption{Volume rendering of an MRI dataset using three cut planes.}
    \label{fig:cutPlanes}
\end{figure}
In the fragment shader, the world space position of each fragment is used to sample the volumetric data set. The sampled value can then be used to determine the final color of the pixel via some transfer function.
This method is interpret and implement in the rasterization pipeline, requiring only a simple modification to the fragment shader. 

\subsubsection{Volume Ray Casting}
Volume Ray Casting is a rendering technique used to directly render volumetric scalar fields.
Intuitively, a ray is projected from each pixel into the volume,
and the scalar field is iteratively sampled along each ray to compute the pixel values.
This algorithm approximates the volume rendering rendering integral by computing the Discrete Volume Rendering Integral (DVRI) 

$$ I(D) = \sum_{i=0}^n C_iA_i\prod_{j=i+1}^{n}(1-A_j)$$
Where $I$ is the accumulated intensity for a given pixel with total ray depth $D$.
$C_i$ and $A_i$ represent the intensities and opacities sampled along the ray, respectively. This
can be extended to color images by performing the same algorithm for each channel in the color image (RGB).

If ray-casting is performed front-to-back, the algorithm takes the following form \cite{Kruger2006GPU}
$$ C_{dst} \leftarrow C_{dst} + (1-\alpha_{dst})C_{src} $$
$$ \alpha_{dst} \leftarrow \alpha_{dst} + (1-\alpha_{dst})\alpha_{src}$$
Here, the \textit{destination} color $C_{dst}$ and opacity $A_{dst}$ are computed by 
\textit{compositing} the result from the previous iteration with the \textit{source} values, sampled at the current ray segment. To sample arbitrary positions with the volume, trilinear interpolation is used. A shading model may also be applied during ray traversal,
and this typically requires computing the gradients of the scalar field.

Transfer functions are used to map data values from the 3D scalar field to colors and opacities. The choice of transfer function
depends on the desired visualization and may require expert insight into the nature of the underlying data.
The steps of the volume ray casting algorithm are demonstrated in figure \ref{fig:volumeRayCasting}.


\newpage
\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Volume_ray_casting.png}
    \caption{Volume ray casting. The generated ray for a given fragment is shown in (1), passing through the discrete scalar field. In (2), data values are sampled along the ray. Then, a transfer function and shading model are applied at each sample (3) to compute color and opacity values. Finally, the pixel value
    is computed through compositing (4) \cite{Hofmann2011}.}
    \label{fig:volumeRayCasting}
\end{figure}

This algorithm can be implemented on the GPU by conforming to the traditional rasterization pipeline by using \textit{proxy geometries}.
The proxy geometries are placed such that the rasterized pixels cover the projection of the scalar field in view space. Thus,
the ray casting algorithm can be implemented directly in the fragment shader used for these geometries. 

\subsection{Isosurface Rendering}
In isosurface rendering, a triangulated mesh is generated directly from the volume data.
The mesh is formed by generating vertices at volume cells whose data includes a specific value.
The mesh can then be rendered using the traditional rasterization pipeline for rendering surfaces, and 
lighting models can be applied to achieve proper shading. This method is generally cheaper in rendering cost as the
geometry is directly rendered through projection and rasterization rather than by direct volume rendering such as with
ray casting.



\bibliography{references}
\end{document}
